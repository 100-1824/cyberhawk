#!/usr/bin/env python3
"""
CyberHawk Malware Behavioral Analyzer - Full Edition
Performs behavioral analysis with API integration
"""

import os
import sys
import json
import re
import argparse
import hashlib
import time
import requests
from datetime import datetime
from pathlib import Path

# Configuration
CONFIG = {
    'DATA_DIR': 'assets/data',
    'UPLOADS_DIR': 'assets/data/malware_uploads',
    'PROGRESS_FILE': 'assets/data/malware_scan_progress.json',
    'REPORTS_FILE': 'assets/data/malware_reports.json',
    # API Keys - set these for real API lookups
    'VIRUSTOTAL_API_KEY': '685fe9d7889aaddde1c019f7d2a4ebccc9032c2663112fdc95257a699b3d4f30',  # Get free key from virustotal.com
    'MALWAREBAZAAR_ENABLED': True,  # MalwareBazaar is free, no key needed
}

def update_progress(file_id, progress, status, stage='scanning'):
    """Update scan progress file for frontend to read"""
    try:
        progress_file = os.path.join(CONFIG['DATA_DIR'], 'malware_scan_progress.json')
        data = {
            'file_id': file_id,
            'progress': progress,
            'status': status,
            'stage': stage,
            'updated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        with open(progress_file, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"[PROGRESS] {progress}% - {status}")
    except Exception as e:
        print(f"[ERROR] Failed to update progress: {e}")

def calculate_file_hash(filepath, algorithm='sha256'):
    """Calculate file hash"""
    hash_func = hashlib.new(algorithm)
    with open(filepath, 'rb') as f:
        while chunk := f.read(8192):
            hash_func.update(chunk)
    return hash_func.hexdigest()

def calculate_all_hashes(filepath):
    """Calculate MD5 and SHA256 hashes for a file"""
    md5_hash = hashlib.md5()
    sha256_hash = hashlib.sha256()
    
    with open(filepath, 'rb') as f:
        while chunk := f.read(8192):
            md5_hash.update(chunk)
            sha256_hash.update(chunk)
    
    return {
        'md5': md5_hash.hexdigest(),
        'sha256': sha256_hash.hexdigest()
    }

def query_virustotal(file_hash):
    """Query VirusTotal API for file hash"""
    api_key = CONFIG.get('VIRUSTOTAL_API_KEY', '')
    if not api_key:
        print("[API] VirusTotal: No API key configured (skipping)")
        return None
    
    try:
        print(f"[API] Querying VirusTotal for hash: {file_hash[:16]}...")
        url = f"https://www.virustotal.com/api/v3/files/{file_hash}"
        headers = {"x-apikey": api_key}
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            stats = data.get('data', {}).get('attributes', {}).get('last_analysis_stats', {})
            print(f"[API] VirusTotal: Found! Malicious: {stats.get('malicious', 0)}, Clean: {stats.get('harmless', 0)}")
            return data
        elif response.status_code == 404:
            print("[API] VirusTotal: File not found in database")
            return {'status': 'not_found'}
        else:
            print(f"[API] VirusTotal: Error {response.status_code}")
            return None
    except Exception as e:
        print(f"[API] VirusTotal error: {e}")
        return None

def query_malwarebazaar(file_hash):
    """Query MalwareBazaar API for file hash - FREE, no API key needed"""
    if not CONFIG.get('MALWAREBAZAAR_ENABLED', True):
        return None
    
    try:
        print(f"[API] Querying MalwareBazaar for hash: {file_hash[:16]}...")
        url = "https://mb-api.abuse.ch/api/v1/"
        data = {"query": "get_info", "hash": file_hash}
        response = requests.post(url, data=data, timeout=15)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('query_status') == 'ok':
                print(f"[API] MalwareBazaar: MALWARE FOUND! Signature: {result.get('data', [{}])[0].get('signature', 'Unknown')}")
                return result
            else:
                print("[API] MalwareBazaar: File not found in database (clean or unknown)")
                return {'query_status': 'not_found'}
        else:
            print(f"[API] MalwareBazaar: Error {response.status_code}")
            return None
    except Exception as e:
        print(f"[API] MalwareBazaar error: {e}")
        return None

class BehavioralAnalyzer:
    def __init__(self):
        self.reports_file = os.path.join(CONFIG['DATA_DIR'], 'malware_reports.json')
        
        # Behavioral patterns for detection
        self.api_patterns = {
            'Process Injection': {
                'apis': ['CreateRemoteThread', 'WriteProcessMemory', 'VirtualAllocEx',
                        'NtCreateThreadEx', 'RtlCreateUserThread', 'SetThreadContext'],
                'risk': 'critical'
            },
            'Code Injection': {
                'apis': ['LoadLibrary', 'GetProcAddress', 'VirtualAlloc',
                        'VirtualProtect', 'CreateThread'],
                'risk': 'high'
            },
            'Keylogging': {
                'apis': ['SetWindowsHookEx', 'GetAsyncKeyState', 'GetKeyState',
                        'RegisterHotKey', 'GetKeyboardState', 'GetRawInputData'],
                'risk': 'critical'
            },
            'Screen Capture': {
                'apis': ['BitBlt', 'GetDC', 'CreateCompatibleDC',
                        'CreateCompatibleBitmap', 'StretchBlt', 'GetDesktopWindow'],
                'risk': 'high'
            },
            'Network Activity': {
                'apis': ['InternetOpen', 'InternetConnect', 'HttpSendRequest',
                        'WSAStartup', 'socket', 'connect', 'send', 'recv'],
                'risk': 'medium'
            },
            'File System': {
                'apis': ['CreateFile', 'WriteFile', 'ReadFile',
                        'DeleteFile', 'MoveFile', 'CopyFile', 'FindFirstFile'],
                'risk': 'low'
            },
            'Registry': {
                'apis': ['RegOpenKey', 'RegSetValue', 'RegDeleteValue',
                        'RegCreateKey', 'RegQueryValue', 'RegEnumKey'],
                'risk': 'medium'
            },
            'Service Control': {
                'apis': ['CreateService', 'OpenService', 'StartService',
                        'ControlService', 'DeleteService', 'ChangeServiceConfig'],
                'risk': 'high'
            },
            'Persistence': {
                'apis': ['SetWindowsHookEx', 'RegSetValue', 'CreateService',
                        'WinExec', 'ShellExecute', 'CreateProcess'],
                'risk': 'high'
            },
            'Cryptography': {
                'apis': ['CryptEncrypt', 'CryptDecrypt', 'CryptAcquireContext',
                        'CryptCreateHash', 'CryptDeriveKey', 'CryptGenKey'],
                'risk': 'medium'
            }
        }
        
        self.suspicious_strings = {
            'Commands': {
                'patterns': ['cmd.exe', 'powershell', 'wscript', 'cscript',
                           'rundll32', 'regsvr32', 'mshta', 'certutil'],
                'risk': 'high'
            },
            'System': {
                'patterns': ['net user', 'net localgroup', 'schtasks', 'at.exe',
                           'taskkill', 'reg add', 'reg delete', 'sc create'],
                'risk': 'high'
            },
            'Credentials': {
                'patterns': ['password', 'passwd', 'pwd', 'credential',
                           'username', 'login', 'admin', 'root'],
                'risk': 'medium'
            },
            'Ransomware': {
                'patterns': ['encrypt', 'decrypt', 'bitcoin', 'wallet',
                           'ransom', 'payment', '.locked', '.encrypted'],
                'risk': 'critical'
            },
            'Network': {
                'patterns': ['http://', 'https://', 'ftp://', 'tcp://',
                           'udp://', 'tor', 'onion', 'proxy'],
                'risk': 'medium'
            },
            'Malware': {
                'patterns': ['backdoor', 'rootkit', 'trojan', 'keylog',
                           'inject', 'payload', 'exploit', 'shellcode'],
                'risk': 'critical'
            }
        }
    
    def analyze_file(self, file_id, filepath):
        """Perform comprehensive behavioral analysis with API lookup and progress updates"""
        print(f"\n{'='*60}")
        print(f"[ANALYZE] Starting analysis: {os.path.basename(filepath)}")
        print(f"{'='*60}")
        
        update_progress(file_id, 5, 'Starting analysis...', 'init')
        time.sleep(0.5)  # Allow progress to be visible
        
        try:
            # Step 1: Calculate file hashes (MD5 and SHA256)
            update_progress(file_id, 10, 'Calculating file hashes...', 'hash')
            time.sleep(0.5)
            hashes = calculate_all_hashes(filepath)
            file_hash = hashes['sha256']  # Use SHA256 for API queries
            md5_hash = hashes['md5']
            file_size = os.path.getsize(filepath)
            filename = os.path.basename(filepath)
            file_ext = os.path.splitext(filename)[1].lower()
            print(f"[HASH] MD5: {md5_hash}")
            print(f"[HASH] SHA256: {file_hash}")
            print(f"[SIZE] {file_size} bytes")
            
            # Step 2: Read file data
            update_progress(file_id, 15, 'Reading file data...', 'read')
            time.sleep(0.3)
            with open(filepath, 'rb') as f:
                file_data = f.read()
            
            # Step 3: Query VirusTotal API
            update_progress(file_id, 20, 'Querying VirusTotal...', 'api_vt')
            time.sleep(0.5)
            vt_result = query_virustotal(file_hash)
            
            # Step 4: Query MalwareBazaar API
            update_progress(file_id, 30, 'Querying MalwareBazaar...', 'api_mb')
            time.sleep(0.5)
            mb_result = query_malwarebazaar(file_hash)
            
            # Step 5: Extract strings
            update_progress(file_id, 40, 'Extracting strings...', 'strings')
            time.sleep(0.5)
            strings = self.extract_strings(file_data)
            print(f"[STRINGS] Extracted {len(strings)} strings")
            
            # Step 6: Analyze API calls
            update_progress(file_id, 50, 'Analyzing API calls...', 'api_analysis')
            time.sleep(0.5)
            api_behaviors = self.analyze_api_calls(file_data)
            print(f"[API] Found {len(api_behaviors)} suspicious behaviors")
            
            # Step 7: Analyze suspicious strings
            update_progress(file_id, 60, 'Analyzing suspicious patterns...', 'pattern_analysis')
            time.sleep(0.5)
            string_behaviors = self.analyze_strings(strings)
            
            # Step 8: Analyze file structure
            update_progress(file_id, 70, 'Analyzing file structure...', 'structure')
            time.sleep(0.5)
            structure_analysis = self.analyze_structure(file_data)
            print(f"[STRUCTURE] Format: {structure_analysis.get('file_format', 'Unknown')}")
            
            # Step 9: Calculate threat indicators
            update_progress(file_id, 80, 'Calculating threat level...', 'threat_calc')
            time.sleep(0.5)
            threat_indicators = self.calculate_threat_indicators(
                api_behaviors, string_behaviors, structure_analysis
            )
            
            # Boost threat score if found in malware databases
            if mb_result and mb_result.get('query_status') == 'ok':
                threat_indicators['threat_score'] = min(100, threat_indicators['threat_score'] + 40)
                threat_indicators['risk_level'] = 'CRITICAL'
                threat_indicators['primary_threats'].append('Known Malware (MalwareBazaar)')
            
            if vt_result and vt_result.get('data', {}).get('attributes', {}).get('last_analysis_stats', {}).get('malicious', 0) > 0:
                malicious_count = vt_result['data']['attributes']['last_analysis_stats']['malicious']
                threat_indicators['threat_score'] = min(100, threat_indicators['threat_score'] + malicious_count * 2)
                if threat_indicators['threat_score'] >= 60:
                    threat_indicators['risk_level'] = 'CRITICAL'
                threat_indicators['primary_threats'].append(f'Detected by {malicious_count} VirusTotal engines')
            
            # Step 10: Generate and save report
            update_progress(file_id, 90, 'Generating final report...', 'report')
            time.sleep(0.5)
            
            # Get VT detection counts
            vt_detections = 0
            vt_total = 70
            if vt_result and vt_result.get('data', {}).get('attributes', {}).get('last_analysis_stats'):
                stats = vt_result['data']['attributes']['last_analysis_stats']
                vt_detections = stats.get('malicious', 0)
                vt_total = sum(stats.values())
            
            # Determine threat level label for frontend
            risk_level = threat_indicators['risk_level']
            if risk_level == 'CRITICAL':
                threat_label = 'MALICIOUS'
            elif risk_level == 'HIGH':
                threat_label = 'SUSPICIOUS'
            elif risk_level == 'MEDIUM':
                threat_label = 'LOW'
            else:
                threat_label = 'CLEAN'
            
            # Compile complete analysis result with all fields for frontend
            analysis_result = {
                'file_id': file_id,
                'filename': filename,
                'file_hash': file_hash,
                'md5': md5_hash,
                'sha256': file_hash,
                'file_size': file_size,
                'file_type': file_ext.upper().replace('.', '') if file_ext else 'Unknown',
                'scan_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'threat_level': threat_label,
                'threat_score': threat_indicators['threat_score'],
                'ml_confidence': min(95, threat_indicators['threat_score'] + 10),
                'malware_probability': threat_indicators['threat_score'],
                'vt_detections': vt_detections,
                'vt_total_engines': vt_total,
                'detection_sources': ', '.join(threat_indicators.get('primary_threats', [])) or 'Behavioral Analysis',
                'entropy': structure_analysis.get('entropy', 0),
                'is_packed': structure_analysis.get('is_packed', False),
                'suspicious_apis': len(api_behaviors),
                'api_behaviors': api_behaviors,
                'string_behaviors': string_behaviors,
                'structure_analysis': structure_analysis,
                'extracted_strings': strings[:100],
                'threat_indicators': threat_indicators,
                'virustotal_result': vt_result,
                'malwarebazaar_result': mb_result,
                'behaviors': []
            }
            
            # Add behaviors to report
            for api_behavior in api_behaviors:
                analysis_result['behaviors'].append({
                    'type': api_behavior['type'],
                    'severity': api_behavior['severity'],
                    'description': api_behavior['description']
                })
            
            for string_behavior in string_behaviors:
                if string_behavior['severity'] in ['high', 'medium']:
                    analysis_result['behaviors'].append({
                        'type': f"{string_behavior['category']} Detection",
                        'severity': string_behavior['severity'],
                        'description': string_behavior['description']
                    })
            
            # Save the report
            self.save_report(file_id, analysis_result)
            
            update_progress(file_id, 100, 'Analysis complete!', 'complete')
            
            print(f"\n{'='*60}")
            print(f"[COMPLETE] Analysis finished!")
            print(f"[RESULT] Threat Level: {threat_indicators['risk_level']}")
            print(f"[RESULT] Threat Score: {threat_indicators['threat_score']}/100")
            print(f"{'='*60}\n")
            
            return analysis_result
            
        except Exception as e:
            print(f"[ERROR] Analysis failed: {e}")
            import traceback
            traceback.print_exc()
            update_progress(file_id, 0, f'Error: {str(e)}', 'error')
            return None
    
    def save_report(self, file_id, report_data):
        """Save or update report in malware_reports.json"""
        try:
            reports = []
            if os.path.exists(self.reports_file):
                with open(self.reports_file, 'r') as f:
                    content = f.read()
                    if content:
                        reports = json.loads(content)
            
            # Remove existing report for this file_id
            reports = [r for r in reports if r.get('file_id') != file_id]
            
            # Add new report
            reports.append(report_data)
            
            # Save reports
            with open(self.reports_file, 'w') as f:
                json.dump(reports, f, indent=2)
            
            # Update stats based on all reports
            self.update_stats(reports)
            
            print(f"[SAVED] Report saved for {report_data.get('filename', file_id)}")
            return True
        except Exception as e:
            print(f"[ERROR] Failed to save report: {e}")
            return False
    
    def update_stats(self, reports):
        """Update malware_stats.json based on current reports"""
        try:
            stats_file = os.path.join(CONFIG['DATA_DIR'], 'malware_stats.json')
            
            total_scans = len(reports)
            malware_detected = 0
            clean_files = 0
            suspicious_files = 0
            
            for report in reports:
                threat_level = report.get('threat_level', 'CLEAN').upper()
                if threat_level == 'MALICIOUS' or threat_level == 'CRITICAL':
                    malware_detected += 1
                elif threat_level == 'SUSPICIOUS' or threat_level == 'HIGH':
                    suspicious_files += 1
                elif threat_level == 'LOW' or threat_level == 'MEDIUM':
                    suspicious_files += 1
                else:
                    clean_files += 1
            
            stats = {
                'total_scans': total_scans,
                'malware_detected': malware_detected,
                'clean_files': clean_files,
                'suspicious_files': suspicious_files,
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            with open(stats_file, 'w') as f:
                json.dump(stats, f, indent=2)
            
            print(f"[STATS] Updated: {total_scans} scans, {malware_detected} malware, {clean_files} clean, {suspicious_files} suspicious")
        except Exception as e:
            print(f"[ERROR] Failed to update stats: {e}")
    
    def extract_strings(self, data, min_length=4):
        """Extract printable strings from binary data"""
        strings = []
        current_string = []
        
        # Limit analysis to first 512KB for performance
        data_sample = data[:524288]
        
        for byte in data_sample:
            if 32 <= byte <= 126:  # Printable ASCII
                current_string.append(chr(byte))
            else:
                if len(current_string) >= min_length:
                    string = ''.join(current_string)
                    if len(string) <= 200:  # Limit string length
                        strings.append(string)
                current_string = []
        
        # Add final string
        if len(current_string) >= min_length:
            strings.append(''.join(current_string))
        
        # Return unique strings
        return list(set(strings))[:1000]
    
    def analyze_api_calls(self, data):
        """Analyze API call patterns"""
        behaviors = []
        data_sample = data[:1048576]  # Analyze first 1MB
        
        for behavior_type, info in self.api_patterns.items():
            detected_apis = []
            
            for api in info['apis']:
                if api.encode() in data_sample or api.encode('utf-16le') in data_sample:
                    detected_apis.append(api)
            
            if detected_apis:
                severity = self._get_severity(info['risk'], len(detected_apis))
                
                behaviors.append({
                    'type': behavior_type,
                    'severity': severity,
                    'risk': info['risk'],
                    'detected_apis': detected_apis,
                    'count': len(detected_apis),
                    'description': f"Detected {len(detected_apis)} APIs related to {behavior_type.lower()}"
                })
        
        return behaviors
    
    def analyze_strings(self, strings):
        """Analyze extracted strings for suspicious patterns"""
        behaviors = []
        
        for category, info in self.suspicious_strings.items():
            detected = []
            
            for string in strings:
                string_lower = string.lower()
                for pattern in info['patterns']:
                    if pattern.lower() in string_lower:
                        if string not in detected:
                            detected.append(string)
                            if len(detected) >= 20:  # Limit per category
                                break
                if len(detected) >= 20:
                    break
            
            if detected:
                severity = self._get_severity(info['risk'], len(detected))
                
                behaviors.append({
                    'category': category,
                    'severity': severity,
                    'risk': info['risk'],
                    'detected_strings': detected[:10],
                    'count': len(detected),
                    'description': f"Found {len(detected)} {category.lower()} indicators"
                })
        
        return behaviors
    
    def analyze_structure(self, data):
        """Analyze file structure"""
        analysis = {
            'is_pe': False,
            'is_packed': False,
            'has_overlay': False,
            'suspicious_sections': [],
            'file_format': 'Unknown'
        }
        
        # Check for common file formats
        if data[:2] == b'MZ':
            analysis['is_pe'] = True
            analysis['file_format'] = 'PE (Windows Executable)'
            
            # Check for packing indicators
            packers = {
                b'UPX': 'UPX',
                b'PECompact': 'PECompact',
                b'ASPack': 'ASPack',
                b'Themida': 'Themida',
                b'VMProtect': 'VMProtect',
                b'MPRESS': 'MPRESS',
                b'Petite': 'Petite',
                b'FSG': 'FSG'
            }
            
            for signature, name in packers.items():
                if signature in data[:8192]:
                    analysis['is_packed'] = True
                    analysis['packer_name'] = name
                    break
            
            # Check for suspicious section names
            try:
                pe_offset = int.from_bytes(data[0x3C:0x40], 'little')
                if pe_offset + 0x18 < len(data):
                    section_count = int.from_bytes(data[pe_offset+6:pe_offset+8], 'little')
                    
                    # Look for unusual section names
                    suspicious_names = [b'UPX', b'.packed', b'.crypto', b'.enc']
                    for name in suspicious_names:
                        if name in data[pe_offset:pe_offset+2048]:
                            analysis['suspicious_sections'].append(name.decode('utf-8', errors='ignore'))
            except:
                pass
            
            # Check for overlay data
            if len(data) > 500000:  # Files over 500KB might have overlay
                analysis['has_overlay'] = True
        
        elif data[:4] == b'\x7fELF':
            analysis['file_format'] = 'ELF (Linux Executable)'
        elif data[:4] == b'%PDF':
            analysis['file_format'] = 'PDF'
        elif data[:2] == b'PK':
            analysis['file_format'] = 'ZIP/Office Document'
        
        return analysis
    
    def _get_severity(self, risk, count):
        """Determine severity based on risk level and count"""
        if risk == 'critical':
            return 'high'
        elif risk == 'high':
            return 'high' if count > 3 else 'medium'
        elif risk == 'medium':
            return 'medium' if count > 5 else 'low'
        else:
            return 'low'
    
    def calculate_threat_indicators(self, api_behaviors, string_behaviors, structure_analysis):
        """Calculate overall threat indicators"""
        indicators = {
            'threat_score': 0,
            'risk_level': 'LOW',
            'primary_threats': [],
            'recommendations': []
        }
        
        # Score from API behaviors
        for behavior in api_behaviors:
            if behavior['risk'] == 'critical':
                indicators['threat_score'] += 10
                indicators['primary_threats'].append(behavior['type'])
            elif behavior['risk'] == 'high':
                indicators['threat_score'] += 7
                if behavior['count'] > 3:
                    indicators['primary_threats'].append(behavior['type'])
            elif behavior['risk'] == 'medium':
                indicators['threat_score'] += 4
        
        # Score from string behaviors
        for behavior in string_behaviors:
            if behavior['risk'] == 'critical':
                indicators['threat_score'] += 8
                indicators['primary_threats'].append(f"{behavior['category']} Strings")
            elif behavior['risk'] == 'high':
                indicators['threat_score'] += 5
            elif behavior['risk'] == 'medium':
                indicators['threat_score'] += 3
        
        # Score from structure
        if structure_analysis['is_packed']:
            indicators['threat_score'] += 10
            indicators['recommendations'].append(
                f"File is packed with {structure_analysis.get('packer_name', 'unknown packer')} - consider unpacking for deeper analysis"
            )
        
        if structure_analysis['has_overlay']:
            indicators['threat_score'] += 5
            indicators['recommendations'].append("File contains overlay data - may hide additional payload")
        
        if structure_analysis['suspicious_sections']:
            indicators['threat_score'] += 7
            indicators['recommendations'].append("Suspicious PE sections detected")
        
        # Cap score at 100
        indicators['threat_score'] = min(indicators['threat_score'], 100)
        
        # Determine risk level
        if indicators['threat_score'] >= 60:
            indicators['risk_level'] = 'CRITICAL'
        elif indicators['threat_score'] >= 40:
            indicators['risk_level'] = 'HIGH'
        elif indicators['threat_score'] >= 20:
            indicators['risk_level'] = 'MEDIUM'
        else:
            indicators['risk_level'] = 'LOW'
        
        # Generate recommendations based on detected threats
        if 'Process Injection' in indicators['primary_threats']:
            indicators['recommendations'].append("Process injection capabilities detected - isolate and monitor")
        
        if 'Keylogging' in indicators['primary_threats']:
            indicators['recommendations'].append("Keylogging capabilities detected - check for data theft")
        
        if 'Network Activity' in [b['type'] for b in api_behaviors]:
            indicators['recommendations'].append("Network capabilities detected - monitor network traffic")
        
        if 'Ransomware Strings' in indicators['primary_threats']:
            indicators['recommendations'].append("Ransomware indicators detected - backup critical data immediately")
        
        # Ensure we have some recommendations
        if not indicators['recommendations']:
            if indicators['risk_level'] in ['CRITICAL', 'HIGH']:
                indicators['recommendations'].append("Execute in isolated sandbox environment only")
            elif indicators['risk_level'] == 'MEDIUM':
                indicators['recommendations'].append("Monitor file behavior in controlled environment")
            else:
                indicators['recommendations'].append("Standard security practices recommended")
        
        return indicators
    
    def update_report(self, file_id, analysis):
        """Update existing report with behavioral analysis"""
        try:
            if not os.path.exists(self.reports_file):
                print("[WARN] Reports file not found")
                return
            
            with open(self.reports_file, 'r') as f:
                content = f.read()
                if not content:
                    reports = []
                else:
                    reports = json.loads(content)
            
            # Find and update report
            updated = False
            for report in reports:
                if report.get('file_id') == file_id:
                    # Merge behavioral analysis into existing report
                    report['behavioral_analysis'] = analysis
                    
                    # Update behaviors list
                    if 'behaviors' not in report:
                        report['behaviors'] = []
                    
                    # Add API behaviors
                    for api_behavior in analysis['api_behaviors']:
                        report['behaviors'].append({
                            'type': api_behavior['type'],
                            'severity': api_behavior['severity'],
                            'description': api_behavior['description']
                        })
                    
                    # Add significant string behaviors
                    for string_behavior in analysis['string_behaviors']:
                        if string_behavior['severity'] in ['high', 'medium']:
                            report['behaviors'].append({
                                'type': f"{string_behavior['category']} Detection",
                                'severity': string_behavior['severity'],
                                'description': string_behavior['description']
                            })
                    
                    # Update threat indicators
                    report['threat_indicators'] = analysis['threat_indicators']
                    
                    # Update strings if more were found
                    if 'strings' not in report or not report['strings']:
                        report['strings'] = analysis['extracted_strings'][:50]
                    
                    updated = True
                    break
            
            if updated:
                with open(self.reports_file, 'w') as f:
                    json.dump(reports, f, indent=2)
                
                print("[UPDATE] ✓ Report updated with behavioral analysis")
            else:
                print(f"[WARN] Report not found for file ID: {file_id}")
            
        except Exception as e:
            print(f"[ERROR] Failed to update report: {e}")
    
    def generate_detailed_report(self, file_id):
        """Generate detailed analysis report"""
        try:
            with open(self.reports_file, 'r') as f:
                reports = json.load(f)
            
            for report in reports:
                if report.get('file_id') == file_id:
                    print("\n" + "="*70)
                    print("DETAILED BEHAVIORAL ANALYSIS REPORT")
                    print("="*70)
                    print(f"File: {report.get('filename', 'Unknown')}")
                    print(f"Threat Level: {report.get('threat_level', 'UNKNOWN')}")
                    
                    if 'behavioral_analysis' in report:
                        analysis = report['behavioral_analysis']
                        
                        print("\n--- API Behaviors ---")
                        for behavior in analysis.get('api_behaviors', []):
                            print(f"  [{behavior['severity'].upper()}] {behavior['type']}")
                            print(f"    Risk: {behavior['risk']}")
                            print(f"    Detected: {behavior['count']} APIs")
                            print(f"    {behavior['description']}")
                        
                        print("\n--- String Behaviors ---")
                        for behavior in analysis.get('string_behaviors', []):
                            print(f"  [{behavior['severity'].upper()}] {behavior['category']}")
                            print(f"    Risk: {behavior['risk']}")
                            print(f"    Found: {behavior['count']} indicators")
                            if behavior.get('detected_strings'):
                                print(f"    Examples: {', '.join(behavior['detected_strings'][:3])}")
                        
                        print("\n--- Structure Analysis ---")
                        structure = analysis.get('structure_analysis', {})
                        print(f"  File Format: {structure.get('file_format', 'Unknown')}")
                        print(f"  Is Packed: {'Yes - ' + structure.get('packer_name', 'Unknown') if structure.get('is_packed') else 'No'}")
                        print(f"  Has Overlay: {'Yes' if structure.get('has_overlay') else 'No'}")
                        if structure.get('suspicious_sections'):
                            print(f"  Suspicious Sections: {', '.join(structure['suspicious_sections'])}")
                        
                        print("\n--- Threat Indicators ---")
                        indicators = analysis.get('threat_indicators', {})
                        print(f"  Threat Score: {indicators.get('threat_score', 0)}/100")
                        print(f"  Risk Level: {indicators.get('risk_level', 'UNKNOWN')}")
                        
                        if indicators.get('primary_threats'):
                            print(f"\n  Primary Threats:")
                            for threat in indicators['primary_threats']:
                                print(f"    • {threat}")
                        
                        if indicators.get('recommendations'):
                            print(f"\n  Recommendations:")
                            for rec in indicators['recommendations']:
                                print(f"    • {rec}")
                    
                    print("="*70 + "\n")
                    return report
            
            print(f"[ERROR] Report not found for file ID: {file_id}")
            return None
            
        except Exception as e:
            print(f"[ERROR] Failed to generate report: {e}")
            return None

def main():
    parser = argparse.ArgumentParser(description='CyberHawk Behavioral Analyzer')
    parser.add_argument('--file-id', required=True, help='File ID to analyze')
    parser.add_argument('--behavioral', action='store_true', help='Run behavioral analysis')
    parser.add_argument('--report', action='store_true', help='Generate detailed report')
    args = parser.parse_args()
    
    analyzer = BehavioralAnalyzer()
    
    print("=" * 70)
    print("CYBERHAWK BEHAVIORAL ANALYZER - ENHANCED EDITION")
    print("=" * 70)
    print("Analysis Modules:")
    print("  ✓ API Call Detection")
    print("  ✓ String Pattern Analysis")
    print("  ✓ Structure Analysis")
    print("  ✓ Packer Detection")
    print("  ✓ Threat Scoring")
    print("=" * 70 + "\n")
    
    # Find file
    file_id = args.file_id
    uploads_dir = CONFIG['UPLOADS_DIR']
    
    target_file = None
    if os.path.exists(uploads_dir):
        for filename in os.listdir(uploads_dir):
            if filename.startswith(file_id + '_'):
                target_file = os.path.join(uploads_dir, filename)
                break
    
    if not target_file or not os.path.exists(target_file):
        print(f"[ERROR] File not found for ID: {file_id}")
        sys.exit(1)
    
    # Run analysis
    if args.behavioral:
        result = analyzer.analyze_file(file_id, target_file)
        if result:
            print(f"\n[SUCCESS] Behavioral analysis complete")
    
    if args.report:
        analyzer.generate_detailed_report(file_id)

if __name__ == '__main__':
    main()